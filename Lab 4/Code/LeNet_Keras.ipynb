{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mml73JLPoJGD"
      },
      "source": [
        "Now that we have completed the implementation in PyTorch, lets also implement it in Keras.\n",
        "It should be easier(by that I mean shorter) than the PyTorch implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q91Iw5G5UteU"
      },
      "source": [
        "## Datset preparation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kpRr2TLLUgHW"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8fvebPJHUlF4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdc4a75b-61fe-446c-d482-55d411aa893a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9nN7zbE9VuGg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "ec84a47f-a8f1-484d-e121-0d094183c967"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7d70d338fb50>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(x_train[5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tAHlgYttUsbD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceadc185-ca2b-4075-83cc-12d8f2dd8f95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n"
          ]
        }
      ],
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zRb_E5WXToF"
      },
      "source": [
        "### Preprocessing:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVuFkzieVJP5"
      },
      "source": [
        "Since there is no information on channels, we need to manually specify that the image has 1 channel. So lets do that for x_train and x_test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DB8T_oovUxZ1"
      },
      "outputs": [],
      "source": [
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dSxtdLPnVAtl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49a2af6b-91a1-4b63-c18c-94058b3722c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28, 1)\n",
            "(60000,)\n",
            "(10000, 28, 28, 1)\n",
            "(10000,)\n"
          ]
        }
      ],
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQcIifeLWd9e"
      },
      "source": [
        "Lets normalize the pixel values as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "InaNwVQfWhOG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "856158c0-b8fe-4d2e-bebe-c4ab7bf06472"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  3]\n",
            "  [ 18]\n",
            "  [ 18]\n",
            "  [ 18]\n",
            "  [126]\n",
            "  [136]\n",
            "  [175]\n",
            "  [ 26]\n",
            "  [166]\n",
            "  [255]\n",
            "  [247]\n",
            "  [127]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [ 30]\n",
            "  [ 36]\n",
            "  [ 94]\n",
            "  [154]\n",
            "  [170]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [225]\n",
            "  [172]\n",
            "  [253]\n",
            "  [242]\n",
            "  [195]\n",
            "  [ 64]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [ 49]\n",
            "  [238]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [251]\n",
            "  [ 93]\n",
            "  [ 82]\n",
            "  [ 82]\n",
            "  [ 56]\n",
            "  [ 39]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [ 18]\n",
            "  [219]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [198]\n",
            "  [182]\n",
            "  [247]\n",
            "  [241]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [ 80]\n",
            "  [156]\n",
            "  [107]\n",
            "  [253]\n",
            "  [253]\n",
            "  [205]\n",
            "  [ 11]\n",
            "  [  0]\n",
            "  [ 43]\n",
            "  [154]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [ 14]\n",
            "  [  1]\n",
            "  [154]\n",
            "  [253]\n",
            "  [ 90]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [139]\n",
            "  [253]\n",
            "  [190]\n",
            "  [  2]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [ 11]\n",
            "  [190]\n",
            "  [253]\n",
            "  [ 70]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [ 35]\n",
            "  [241]\n",
            "  [225]\n",
            "  [160]\n",
            "  [108]\n",
            "  [  1]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [ 81]\n",
            "  [240]\n",
            "  [253]\n",
            "  [253]\n",
            "  [119]\n",
            "  [ 25]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [ 45]\n",
            "  [186]\n",
            "  [253]\n",
            "  [253]\n",
            "  [150]\n",
            "  [ 27]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [ 16]\n",
            "  [ 93]\n",
            "  [252]\n",
            "  [253]\n",
            "  [187]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [249]\n",
            "  [253]\n",
            "  [249]\n",
            "  [ 64]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [ 46]\n",
            "  [130]\n",
            "  [183]\n",
            "  [253]\n",
            "  [253]\n",
            "  [207]\n",
            "  [  2]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [ 39]\n",
            "  [148]\n",
            "  [229]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [250]\n",
            "  [182]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [ 24]\n",
            "  [114]\n",
            "  [221]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [201]\n",
            "  [ 78]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [ 23]\n",
            "  [ 66]\n",
            "  [213]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [198]\n",
            "  [ 81]\n",
            "  [  2]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [ 18]\n",
            "  [171]\n",
            "  [219]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [195]\n",
            "  [ 80]\n",
            "  [  9]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [ 55]\n",
            "  [172]\n",
            "  [226]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [244]\n",
            "  [133]\n",
            "  [ 11]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [136]\n",
            "  [253]\n",
            "  [253]\n",
            "  [253]\n",
            "  [212]\n",
            "  [135]\n",
            "  [132]\n",
            "  [ 16]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]\n",
            "\n",
            " [[  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]]]\n"
          ]
        }
      ],
      "source": [
        "print(x_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5wuDgMxXaLb"
      },
      "source": [
        "**Normalization**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OfDdyycVVcjf"
      },
      "outputs": [],
      "source": [
        "x_train = x_train / 255\n",
        "x_test = x_test / 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "id": "vuztoP-MWPAH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a54ad4c-522b-4a04-ca7c-99e048ecfb8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.01176471]\n",
            "  [0.07058824]\n",
            "  [0.07058824]\n",
            "  [0.07058824]\n",
            "  [0.49411765]\n",
            "  [0.53333333]\n",
            "  [0.68627451]\n",
            "  [0.10196078]\n",
            "  [0.65098039]\n",
            "  [1.        ]\n",
            "  [0.96862745]\n",
            "  [0.49803922]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.11764706]\n",
            "  [0.14117647]\n",
            "  [0.36862745]\n",
            "  [0.60392157]\n",
            "  [0.66666667]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.88235294]\n",
            "  [0.6745098 ]\n",
            "  [0.99215686]\n",
            "  [0.94901961]\n",
            "  [0.76470588]\n",
            "  [0.25098039]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.19215686]\n",
            "  [0.93333333]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.98431373]\n",
            "  [0.36470588]\n",
            "  [0.32156863]\n",
            "  [0.32156863]\n",
            "  [0.21960784]\n",
            "  [0.15294118]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.07058824]\n",
            "  [0.85882353]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.77647059]\n",
            "  [0.71372549]\n",
            "  [0.96862745]\n",
            "  [0.94509804]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.31372549]\n",
            "  [0.61176471]\n",
            "  [0.41960784]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.80392157]\n",
            "  [0.04313725]\n",
            "  [0.        ]\n",
            "  [0.16862745]\n",
            "  [0.60392157]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.05490196]\n",
            "  [0.00392157]\n",
            "  [0.60392157]\n",
            "  [0.99215686]\n",
            "  [0.35294118]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.54509804]\n",
            "  [0.99215686]\n",
            "  [0.74509804]\n",
            "  [0.00784314]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.04313725]\n",
            "  [0.74509804]\n",
            "  [0.99215686]\n",
            "  [0.2745098 ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.1372549 ]\n",
            "  [0.94509804]\n",
            "  [0.88235294]\n",
            "  [0.62745098]\n",
            "  [0.42352941]\n",
            "  [0.00392157]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.31764706]\n",
            "  [0.94117647]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.46666667]\n",
            "  [0.09803922]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.17647059]\n",
            "  [0.72941176]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.58823529]\n",
            "  [0.10588235]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.0627451 ]\n",
            "  [0.36470588]\n",
            "  [0.98823529]\n",
            "  [0.99215686]\n",
            "  [0.73333333]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.97647059]\n",
            "  [0.99215686]\n",
            "  [0.97647059]\n",
            "  [0.25098039]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.18039216]\n",
            "  [0.50980392]\n",
            "  [0.71764706]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.81176471]\n",
            "  [0.00784314]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.15294118]\n",
            "  [0.58039216]\n",
            "  [0.89803922]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.98039216]\n",
            "  [0.71372549]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.09411765]\n",
            "  [0.44705882]\n",
            "  [0.86666667]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.78823529]\n",
            "  [0.30588235]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.09019608]\n",
            "  [0.25882353]\n",
            "  [0.83529412]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.77647059]\n",
            "  [0.31764706]\n",
            "  [0.00784314]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.07058824]\n",
            "  [0.67058824]\n",
            "  [0.85882353]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.76470588]\n",
            "  [0.31372549]\n",
            "  [0.03529412]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.21568627]\n",
            "  [0.6745098 ]\n",
            "  [0.88627451]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.95686275]\n",
            "  [0.52156863]\n",
            "  [0.04313725]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.53333333]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.83137255]\n",
            "  [0.52941176]\n",
            "  [0.51764706]\n",
            "  [0.0627451 ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]]\n"
          ]
        }
      ],
      "source": [
        "print(x_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GqER3qrcWvOW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1894257f-5403-48f2-bded-62f2a32fd7ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.uint8(5)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "y_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66XsQvABW1E9"
      },
      "source": [
        "**One Hot Encoding**:\n",
        "\n",
        "As we can see, the target labels are the actual values.\n",
        "The output of the final layer of our architecture is a vector of 10 values.\n",
        "\n",
        "So to make them comparable to calculate the loss, lets do `one-hot-encoding` for target labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ljzgf4U_XH3N"
      },
      "outputs": [],
      "source": [
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9Lnur4Z9XKJ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "806d3288-a35e-4e68-892a-d24e16ff0cc8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "y_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk_u3-j4XMcd"
      },
      "source": [
        "## LeNet Model Architecture:\n",
        "\n",
        "The documentation specifies the following parameters to be provided:\n",
        "\n",
        "tf.keras.layers.Conv2D( \\\\\n",
        "    filters,   \\\\\n",
        "    kernel_size, \\\\\n",
        "    strides=(1, 1), \\\\\n",
        "    padding='valid', \\\\\n",
        "    data_format=None, \\\\\n",
        "    dilation_rate=(1, 1), \\\\\n",
        "    groups=1, \\\\\n",
        "    activation=None, \\\\\n",
        "    use_bias=True, \\\\\n",
        "    kernel_initializer='glorot_uniform', \\\\\n",
        "    bias_initializer='zeros', \\\\\n",
        "    kernel_regularizer=None, \\\\\n",
        "    bias_regularizer=None, \\\\\n",
        "    activity_regularizer=None, \\\\\n",
        "    kernel_constraint=None, \\\\\n",
        "    bias_constraint=None, \\\\\n",
        "    **kwargs \\\\\n",
        ")`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "id": "WgwNPIv6pQBm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5753fbf5-5d06-4486-b1fe-ec1a861dba2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_WSSU4bBWBp_"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from keras.layers import MaxPool2D\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Dense, Flatten\n",
        "from keras.models import Sequential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReRC1XINp8cq"
      },
      "source": [
        "### Model Building:\n",
        "\n",
        "Lets make a model1 with relu activation function in cnn layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UaeJWY0PZz5l"
      },
      "outputs": [],
      "source": [
        "model1 = Sequential()\n",
        "\n",
        "model1.add(Conv2D(filters = 6, kernel_size=(5,5), strides = (1,1), padding = 'valid', activation ='relu'))\n",
        "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model1.add(Conv2D(filters = 16, kernel_size=(5, 5), activation='relu'))\n",
        "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model1.add(Flatten())\n",
        "\n",
        "model1.add(Dense(120, activation='relu'))\n",
        "model1.add(Dense(84, activation='relu'))\n",
        "model1.add(Dense(10, activation='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuzIT0RUrtrj"
      },
      "source": [
        "### Model Compilation:\n",
        "\n",
        "Remember the mnemonic `LOM` from the ANN_Tensorflow.ipynb? LOM as in Loss, optimizer, metrics have to be specified during compilation.\n",
        "\n",
        "Other parameters can be seen in the documentation: https://keras.io/api/models/model_training_apis/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "z9JbD7S5qz_X"
      },
      "outputs": [],
      "source": [
        "from keras.metrics import categorical_crossentropy\n",
        "from keras import optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "tJ8v8Thuqh6E"
      },
      "outputs": [],
      "source": [
        "model1.compile(\n",
        "    loss = categorical_crossentropy,\n",
        "    optimizer = optimizers.Adam(),\n",
        "    metrics = ['Accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79QOn0eurwmU"
      },
      "source": [
        "### Model Training / Fitting:\n",
        "\n",
        "We must specify features, labels, batch size, epochs as parameters. Additional parameters can be referred in the documentation as well.\n",
        "\n",
        "The parameter `validation_data` is the data on which to evaluate the loss and any model metrics at the end of each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "WJxNdAX0rSJz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f26d10f-1f71-4cbe-d17c-c0c55194a992"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - Accuracy: 0.8011 - loss: 0.6883 - val_Accuracy: 0.9639 - val_loss: 0.1115\n",
            "Epoch 2/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - Accuracy: 0.9706 - loss: 0.0967 - val_Accuracy: 0.9805 - val_loss: 0.0598\n",
            "Epoch 3/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - Accuracy: 0.9807 - loss: 0.0639 - val_Accuracy: 0.9805 - val_loss: 0.0582\n",
            "Epoch 4/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Accuracy: 0.9838 - loss: 0.0514 - val_Accuracy: 0.9832 - val_loss: 0.0502\n",
            "Epoch 5/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Accuracy: 0.9870 - loss: 0.0412 - val_Accuracy: 0.9878 - val_loss: 0.0377\n",
            "Epoch 6/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - Accuracy: 0.9888 - loss: 0.0363 - val_Accuracy: 0.9892 - val_loss: 0.0346\n",
            "Epoch 7/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - Accuracy: 0.9906 - loss: 0.0293 - val_Accuracy: 0.9888 - val_loss: 0.0353\n",
            "Epoch 8/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - Accuracy: 0.9922 - loss: 0.0249 - val_Accuracy: 0.9888 - val_loss: 0.0361\n",
            "Epoch 9/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Accuracy: 0.9930 - loss: 0.0216 - val_Accuracy: 0.9873 - val_loss: 0.0402\n",
            "Epoch 10/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - Accuracy: 0.9938 - loss: 0.0180 - val_Accuracy: 0.9895 - val_loss: 0.0339\n",
            "Epoch 11/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - Accuracy: 0.9953 - loss: 0.0151 - val_Accuracy: 0.9905 - val_loss: 0.0308\n",
            "Epoch 12/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - Accuracy: 0.9947 - loss: 0.0152 - val_Accuracy: 0.9898 - val_loss: 0.0354\n",
            "Epoch 13/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Accuracy: 0.9956 - loss: 0.0136 - val_Accuracy: 0.9868 - val_loss: 0.0461\n",
            "Epoch 14/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - Accuracy: 0.9962 - loss: 0.0107 - val_Accuracy: 0.9874 - val_loss: 0.0453\n",
            "Epoch 15/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - Accuracy: 0.9959 - loss: 0.0119 - val_Accuracy: 0.9878 - val_loss: 0.0437\n",
            "Epoch 16/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - Accuracy: 0.9959 - loss: 0.0123 - val_Accuracy: 0.9894 - val_loss: 0.0411\n",
            "Epoch 17/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - Accuracy: 0.9972 - loss: 0.0087 - val_Accuracy: 0.9883 - val_loss: 0.0458\n",
            "Epoch 18/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - Accuracy: 0.9975 - loss: 0.0082 - val_Accuracy: 0.9889 - val_loss: 0.0452\n",
            "Epoch 19/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Accuracy: 0.9970 - loss: 0.0086 - val_Accuracy: 0.9895 - val_loss: 0.0393\n",
            "Epoch 20/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - Accuracy: 0.9974 - loss: 0.0072 - val_Accuracy: 0.9884 - val_loss: 0.0467\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d70d1285590>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "model1.fit(x_train, y_train, batch_size = 128, epochs = 20, verbose = 1, validation_data = (x_test, y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWZd0JLjtyeC"
      },
      "source": [
        "### Model Prediction / Evaluation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-Ix60GQxMeW"
      },
      "source": [
        "**Prediction**:\n",
        "\n",
        "Lets try to predict on a sample instance and see if its correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zuUZejlwvS_8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "928c52e5-a9ae-4e24-e6df-9c0df560cb37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 469ms/step\n",
            "[[1.4501036e-12 9.3611052e-10 1.8412342e-10 1.4302480e-10 1.9942874e-11\n",
            "  8.3851480e-13 1.7023140e-18 1.0000000e+00 1.0895612e-12 2.7182812e-08]]\n",
            "7\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "instance = np.expand_dims(x_test[0], axis = 0)\n",
        "output = model1.predict(instance)\n",
        "print(output)\n",
        "print(np.argmax(output))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Kl5wbad7wc0A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc390071-bde3-45a8-ac79-4771f52c0190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "7\n"
          ]
        }
      ],
      "source": [
        "print(y_test[0])\n",
        "print(np.argmax(y_test[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOoDHeMQxTYE"
      },
      "source": [
        "Thus the prediction is correct."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL4QXVGWwsZH"
      },
      "source": [
        "**Evaluation**:\n",
        "\n",
        "To evaluate the overall model on test dataset, we use `model.evaluate`, which requires the features (x_test) and labels (y_test) to be passed as parameters. Other additional parameters can be supplied such as batch_size, steps, etc.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-Sah1QMXs7Er",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f10c7c62-62b5-42f9-825c-e0f5280bb633"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - Accuracy: 0.9863 - loss: 0.0602\n"
          ]
        }
      ],
      "source": [
        "score = model1.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ZKyo7MN2u4UF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25b80a0d-acfa-41e5-8663-784fb774133d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "print(type(score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "57t-s2KauxDl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "838fb5e6-fc2f-4286-d084-1e01ed5ae5c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.04668277129530907\n",
            "Test accuracy: 0.9883999824523926\n"
          ]
        }
      ],
      "source": [
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQAimj3Yxvqk"
      },
      "source": [
        "Thus, the LeNet architecture achieves very high accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awb2OfifxYck"
      },
      "source": [
        "## Modified LeNet Architecture:\n",
        "\n",
        "Notice that we used `relu` activation in the above LeNet architecture. So what if we use `tanh` instead? Lets quickly modify the architecture by using tanh activation function and go through the same steps and compare the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "hz1zWa8bxu3h"
      },
      "outputs": [],
      "source": [
        "model2 = Sequential()\n",
        "\n",
        "model2.add(Conv2D(filters = 6, kernel_size=(5,5), strides = (1,1), padding = 'valid', activation ='tanh'))\n",
        "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model2.add(Conv2D(filters = 16, kernel_size=(5, 5), activation='tanh'))\n",
        "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model2.add(Flatten())\n",
        "\n",
        "model2.add(Dense(120, activation='relu'))\n",
        "model2.add(Dense(84, activation='relu'))\n",
        "model2.add(Dense(10, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ver0LAC4x-0_"
      },
      "outputs": [],
      "source": [
        "model2.compile(\n",
        "    loss = categorical_crossentropy,\n",
        "    optimizer = optimizers.Adam(),\n",
        "    metrics = ['Accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "uWZoDvwZyTuR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df443684-9f33-47f9-e3c8-caff6ec5dd83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - Accuracy: 0.8088 - loss: 0.6527 - val_Accuracy: 0.9731 - val_loss: 0.0859\n",
            "Epoch 2/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Accuracy: 0.9726 - loss: 0.0881 - val_Accuracy: 0.9800 - val_loss: 0.0613\n",
            "Epoch 3/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - Accuracy: 0.9809 - loss: 0.0593 - val_Accuracy: 0.9817 - val_loss: 0.0582\n",
            "Epoch 4/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - Accuracy: 0.9861 - loss: 0.0423 - val_Accuracy: 0.9846 - val_loss: 0.0458\n",
            "Epoch 5/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - Accuracy: 0.9887 - loss: 0.0361 - val_Accuracy: 0.9849 - val_loss: 0.0477\n",
            "Epoch 6/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Accuracy: 0.9915 - loss: 0.0268 - val_Accuracy: 0.9858 - val_loss: 0.0442\n",
            "Epoch 7/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - Accuracy: 0.9922 - loss: 0.0236 - val_Accuracy: 0.9880 - val_loss: 0.0411\n",
            "Epoch 8/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Accuracy: 0.9940 - loss: 0.0181 - val_Accuracy: 0.9869 - val_loss: 0.0458\n",
            "Epoch 9/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - Accuracy: 0.9946 - loss: 0.0160 - val_Accuracy: 0.9866 - val_loss: 0.0463\n",
            "Epoch 10/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - Accuracy: 0.9960 - loss: 0.0127 - val_Accuracy: 0.9866 - val_loss: 0.0484\n",
            "Epoch 11/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - Accuracy: 0.9965 - loss: 0.0108 - val_Accuracy: 0.9870 - val_loss: 0.0467\n",
            "Epoch 12/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Accuracy: 0.9961 - loss: 0.0113 - val_Accuracy: 0.9870 - val_loss: 0.0465\n",
            "Epoch 13/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - Accuracy: 0.9974 - loss: 0.0083 - val_Accuracy: 0.9876 - val_loss: 0.0506\n",
            "Epoch 14/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - Accuracy: 0.9964 - loss: 0.0105 - val_Accuracy: 0.9863 - val_loss: 0.0554\n",
            "Epoch 15/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Accuracy: 0.9970 - loss: 0.0086 - val_Accuracy: 0.9858 - val_loss: 0.0592\n",
            "Epoch 16/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - Accuracy: 0.9974 - loss: 0.0077 - val_Accuracy: 0.9855 - val_loss: 0.0543\n",
            "Epoch 17/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - Accuracy: 0.9977 - loss: 0.0073 - val_Accuracy: 0.9860 - val_loss: 0.0571\n",
            "Epoch 18/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Accuracy: 0.9969 - loss: 0.0081 - val_Accuracy: 0.9868 - val_loss: 0.0573\n",
            "Epoch 19/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - Accuracy: 0.9978 - loss: 0.0056 - val_Accuracy: 0.9867 - val_loss: 0.0609\n",
            "Epoch 20/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - Accuracy: 0.9977 - loss: 0.0070 - val_Accuracy: 0.9893 - val_loss: 0.0523\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d7105bf6390>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "model2.fit(x_train, y_train, batch_size = 128, epochs = 20, verbose = 1, validation_data = (x_test, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1RtP9QHXyYd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa9138b1-f052-402d-8364-8f470fa664b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - Accuracy: 0.9859 - loss: 0.0694\n"
          ]
        }
      ],
      "source": [
        "score2 = model2.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "n-BegPyyyrQq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeaee627-306f-450d-8f72-8f50653a246a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.052274126559495926\n",
            "Test accuracy: 0.989300012588501\n"
          ]
        }
      ],
      "source": [
        "print(\"Test loss:\", score2[0])\n",
        "print(\"Test accuracy:\", score2[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UDUQ5f7zEAb"
      },
      "source": [
        "Accuracy doesn't vary as much. You may try to experiment some more with the architecture as well."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}